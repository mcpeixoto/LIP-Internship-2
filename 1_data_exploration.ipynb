{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.3 64-bit"
  },
  "interpreter": {
   "hash": "4e1d9a8909477db77738c33245c29c7265277ef753467dede8cf3f814cde494e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n  %reload_ext autotime\ntime: 30.8 ms (started: 2021-06-17 17:46:12 +01:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from os.path import basename\n",
    "from config import bkg_data_path, signal_data_path, processed_data_path\n",
    "import time\n",
    "import mplhep as hep\n",
    "%load_ext autotime"
   ]
  },
  {
   "source": [
    "# Sanity checks\n",
    "### Listing all the files on the bkg and signal directory (file path)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Signal files: 5 \nBackground files: 18 \n> Total: 23\ntime: 2.49 ms (started: 2021-06-17 16:43:35 +01:00)\n"
     ]
    }
   ],
   "source": [
    "bkg_files = glob.glob(join(bkg_data_path, \"*.*\"))\n",
    "signal_files = glob.glob(join(signal_data_path, \"*/*.*\"))\n",
    "all_files = bkg_files + signal_files\n",
    "\n",
    "print(\"Signal files:\", len(signal_files), \"\\nBackground files:\", len(bkg_files), \"\\n> Total:\", len(all_files))"
   ]
  },
  {
   "source": [
    "## Checking features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = {}\n",
    "for path in all_files:\n",
    "    # Load data\n",
    "    if path.endswith(\".csv\"):\n",
    "        data = pd.read_csv(path)\n",
    "    elif path.endswith(\".h5\"):\n",
    "        data = pd.read_hdf(path)\n",
    "\n",
    "    # Get features\n",
    "    features = list(data.columns)\n",
    "\n",
    "    file_name = basename(path)\n",
    "    for feature in features:\n",
    "        if feature not in book:\n",
    "            book[feature] = []\n",
    "        book[feature] += [file_name]\n",
    "\n",
    "    # Saving memory\n",
    "    del data"
   ]
  },
  {
   "source": [
    "After getting a directory with the structure:\n",
    "- {feature:\\[name_of_file\\]}\n",
    "\n",
    "We can compare each of the files features to see if they match"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nFeature \"gen_sample\" is missing on 1 file(s).\n-> Files that are missing the feature:\n\t {'tZFCNC.h5'}\n\nFeature \"gen_filter\" is missing on 1 file(s).\n-> Files that are missing the feature:\n\t {'tZFCNC.h5'}\n\nFeature \"gen_decay_filter\" is missing on 1 file(s).\n-> Files that are missing the feature:\n\t {'tZFCNC.h5'}\n\nFeature \"MissingET_Eta\" is missing on 22 file(s).\n-> Files that are missing the feature:\n\t {'WW_2L_PTW0to250.csv', 'ZZ_2L_PTZ500.csv', 'Zbb_2L_HT0to250.csv', 'Zbb_2L_HT250to500.csv', 'ZZ_2L_PTZ0to250.csv', 'Zjj_2L_HT500.csv', 'Zbb_2L_HT500.csv', 'mch45_HG_13TeV_HG3000_HQ1000_train.csv', 'ZZ_2L_PTZ250to500.csv', 'WW_2L_PTW500.csv', 'WZ_2L_PTZ500.csv', 'WW_2L_PTW250to500.csv', 'mch45_HG_13TeV_HG3000_HQ1000_test.csv', 'mch45_HG_13TeV_wohg_HQ1000_train.csv', 'WZ_2L_PTZ0to250.csv', 'mch45_HG_13TeV_wohg_HQ1000_test.csv', 'WZ_2L_PTZ250to500.csv', 'Zjj_2L_HT250to500.csv', 'ttbar_2L_PTtop0to100.csv', 'ttbar_2L_PTtop100to250.csv', 'Zjj_2L_HT0to250.csv', 'ttbar_2L_PTtop250.csv'}\n"
     ]
    }
   ],
   "source": [
    "for x in book:\n",
    "    if len(book[x]) != len(all_files):\n",
    "        print(f\"\\nFeature \\\"{x}\\\" is missing on\",  len(all_files)-len(book[x]), \"file(s).\")\n",
    "        print(\"-> Files that are missing the feature:\\n\\t\", set([basename(x) for x in all_files]) - set(book[x]))"
   ]
  },
  {
   "source": [
    "Since the features highlighted by out little script are irrelevant, we can just mass-delete them on all the files when we pre-process the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Pre-Processing\n",
    "-> Delete irrelevant features (ex: features that were used on the simulation of the data)\n",
    "\n",
    "-> Apply Cuts\n",
    "\n",
    "- At least two final state leptons\n",
    "\n",
    "- At least one b-tagged jet\n",
    "\n",
    "- Large scalar sum of transverse momentum (p_t), H_t > 500 GeV\n",
    "\n",
    "-> Weight the data\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 352 µs (started: 2021-06-17 16:43:55 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# This features shall be removed\n",
    "to_remove = [\n",
    "        'gen_xsec',\n",
    "        'gen_decay1',\n",
    "        'gen_decay2',\n",
    "        'gen_sample',\n",
    "        'gen_filter',\n",
    "        'gen_decay_filter',\n",
    "        'MissingET_Eta'\n",
    "            ]"
   ]
  },
  {
   "source": [
    "### Pre-process and join all the background data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "time: 798 µs (started: 2021-06-17 16:43:55 +01:00)\n"
     ]
    }
   ],
   "source": [
    "def pre_process(paths):\n",
    "    print(\"[Info] Started preprocessing..\")\n",
    "    keys = set([x.split(\"/\")[-2] for x in all_files])\n",
    "    print(f\"[Info] Keys: {keys}\\n\")\n",
    "\n",
    "    for i, path in enumerate(tqdm(paths)):\n",
    "        # Load data\n",
    "        if path.endswith(\".csv\"):\n",
    "            data = pd.read_csv(path)\n",
    "        elif path.endswith(\".h5\"):\n",
    "            data = pd.read_hdf(path)\n",
    "\n",
    "        # Add a column with the file name for later reference\n",
    "        data[\"name\"] = basename(path)\n",
    "\n",
    "        # Weights\n",
    "        data[\"gen_xsec\"] = data[\"gen_xsec\"].mean() / data.shape[0]\n",
    "        data.rename(columns={\"gen_xsec\":\"weights\"}, inplace=True)\n",
    "\n",
    "        # Remove ussless features\n",
    "        for x in to_remove:\n",
    "            try:\n",
    "                data.drop([x], axis=1, inplace=True)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Statistical purposes\n",
    "        shape_before = data.shape[0]\n",
    "\n",
    "        ## Apply Cuts\n",
    "        # At least 2 leptons\n",
    "        data = data[(data['Electron_Multi'] + data['Muon_Multi']) >= 2] \n",
    "        # At least 1 B-Tag\n",
    "        data = data[(data['Jet1_BTag'] + data['Jet2_BTag'] + data['Jet3_BTag'] + data['Jet4_BTag'] + data['Jet5_BTag']) >= 1]\n",
    "        # H_T > 500 GeV\n",
    "        data = data[data['ScalarHT_HT'] > 500]\n",
    "\n",
    "        # Statistical purposes\n",
    "        shape_after = data.shape[0]\n",
    "\n",
    "        print(f\"[Info] Data Reduction for \\\"{basename(path)}\\\": {int(((shape_before-shape_after)/shape_before)*100)}%\")\n",
    "\n",
    "\n",
    "        ## SAVING\n",
    "        save_path = join(processed_data_path,path.split(\"/\")[-2] + \".csv\")\n",
    "\n",
    "        # Create / Reset file on first iteration # TODO: COMPOR\n",
    "        if i == 0:\n",
    "            with open(save_path, 'w') as fp:\n",
    "                pass\n",
    "\n",
    "        with open(save_path, 'a') as f:\n",
    "            data.to_csv(f, header=f.tell()==0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Info] Started preprocessing..\n[Info] Keys: {'VLQ_HG', 'VLQ_SEM_HG', 'bkg', 'FCNC'}\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/23 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3519f5df9f9f429f9cae30c1f3e5e97f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Info] Data Reduction for \"ttbar_2L_PTtop0to100.csv\": 98%\n",
      "[Info] Data Reduction for \"ttbar_2L_PTtop100to250.csv\": 96%\n",
      "[Info] Data Reduction for \"ttbar_2L_PTtop250.csv\": 83%\n",
      "[Info] Data Reduction for \"WW_2L_PTW0to250.csv\": 99%\n",
      "[Info] Data Reduction for \"WW_2L_PTW250to500.csv\": 99%\n",
      "[Info] Data Reduction for \"WW_2L_PTW500.csv\": 98%\n",
      "[Info] Data Reduction for \"WZ_2L_PTZ0to250.csv\": 99%\n",
      "[Info] Data Reduction for \"WZ_2L_PTZ250to500.csv\": 94%\n",
      "[Info] Data Reduction for \"WZ_2L_PTZ500.csv\": 93%\n",
      "[Info] Data Reduction for \"Zbb_2L_HT0to250.csv\": 99%\n",
      "[Info] Data Reduction for \"Zbb_2L_HT250to500.csv\": 73%\n",
      "[Info] Data Reduction for \"Zbb_2L_HT500.csv\": 61%\n",
      "[Info] Data Reduction for \"Zjj_2L_HT0to250.csv\": 99%\n",
      "[Info] Data Reduction for \"Zjj_2L_HT250to500.csv\": 96%\n",
      "[Info] Data Reduction for \"Zjj_2L_HT500.csv\": 94%\n",
      "[Info] Data Reduction for \"ZZ_2L_PTZ0to250.csv\": 99%\n",
      "[Info] Data Reduction for \"ZZ_2L_PTZ250to500.csv\": 89%\n",
      "[Info] Data Reduction for \"ZZ_2L_PTZ500.csv\": 90%\n",
      "[Info] Data Reduction for \"tZFCNC.h5\": 79%\n",
      "[Info] Data Reduction for \"mch45_HG_13TeV_HG3000_HQ1000_test.csv\": 94%\n",
      "[Info] Data Reduction for \"mch45_HG_13TeV_HG3000_HQ1000_train.csv\": 94%\n",
      "[Info] Data Reduction for \"mch45_HG_13TeV_wohg_HQ1000_test.csv\": 94%\n",
      "[Info] Data Reduction for \"mch45_HG_13TeV_wohg_HQ1000_train.csv\": 94%\n",
      "time: 4min 11s (started: 2021-06-17 16:43:56 +01:00)\n"
     ]
    }
   ],
   "source": [
    "pre_process(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}