{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning import Trainer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from typing import Optional\n",
    "from config import processed_data_path\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Importing aux functions\n",
    "from aux import _dataset"
   ]
  },
  {
   "source": [
    "## Defining the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, dataset, batch_size, hidden_size, alpha, lr):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - > variant e {'VLQ_HG', 'VLQ_SEM_HG', 'bkg', 'FCNC'}; it's the type of data\n",
    "        - > hidden_size : Latent Hidden Size\n",
    "        - > alpha : Hyperparameter to control the importance of\n",
    "        reconstruction loss vs KL-Divergence Loss\n",
    "        - > lr : Learning Rate, will not be used if auto_lr_find is used.\n",
    "        - > dataset : Dataset to used\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.hparams.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(69, 128), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(128, hidden_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.hidden2mu = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden2log_var = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(128, 69), \n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Pass through encoder\n",
    "        out = self.encoder(x)\n",
    "        mu = self.hidden2mu(out)\n",
    "        log_var = self.hidden2log_var(out)\n",
    "        return mu, log_var\n",
    "\n",
    "    def decode(self, x):\n",
    "        # Pass through encoder\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def reparametrize(self, mu, log_var):\n",
    "        # Reparametrization Trick\n",
    "        # It outputs a sample of the dist.\n",
    "        # mu -> average | log_var -> std\n",
    "        \n",
    "        log_var = torch.exp(0.5*log_var)\n",
    "        z = torch.randn(size=(mu.size(0), mu.size(1))) # log_var, normal distribution\n",
    "        z = z.type_as(mu)\n",
    "        return mu + log_var*z\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through encoder\n",
    "        mu, log_var = self.encode(x)\n",
    "        # Reparametrization Trick\n",
    "        hidden = self.reparametrize(mu, log_var)\n",
    "        # Pass through decoder\n",
    "        output = self.decoder(hidden)\n",
    "\n",
    "        return mu, log_var, output, hidden\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, weights = batch\n",
    "        # Pass\n",
    "        mu, log_var, x_out, _ = self.forward(x)\n",
    "\n",
    "        # Losses\n",
    "        kl_loss = (-0.5*(1+log_var - mu**2 -\n",
    "                         torch.exp(log_var)).sum(dim=1)).mean(dim=0)\n",
    "\n",
    "        recon_loss_criterion = nn.MSELoss()\n",
    "        recon_loss = recon_loss_criterion(x, x_out)\n",
    "\n",
    "        loss = recon_loss*self.alpha + kl_loss\n",
    "\n",
    "        # Weights on final loss\n",
    "        loss = (weights * loss) / weights.sum()\n",
    "        loss = torch.mean(loss, dtype=torch.float32)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, weights = batch\n",
    "\n",
    "        mu, log_var, x_out, _ = self.forward(x)\n",
    "\n",
    "        # K-L Loss\n",
    "        #kl_loss = (-0.5*(1+torch.log(log_var**2)-log_var**2 - mu**2).sum(dim=1)).mean(dim=0) \n",
    "        kl_loss = (-0.5*(1+log_var - mu**2 -\n",
    "                         torch.exp(log_var)).sum(dim=1)).mean(dim=0)\n",
    "        # Weights on KL Loss\n",
    "        kl_loss = (weights * kl_loss) / weights.sum()\n",
    "        kl_loss = torch.mean(kl_loss, dtype=torch.float32)\n",
    "\n",
    "        # Reconstruction loss\n",
    "        recon_loss_criterion = nn.MSELoss()\n",
    "        recon_loss = recon_loss_criterion(x, x_out)\n",
    "        # Weights on recon loss\n",
    "        recon_loss = (weights * recon_loss) / weights.sum()\n",
    "        recon_loss = torch.mean(recon_loss, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        loss = recon_loss*self.alpha + kl_loss\n",
    "\n",
    "        self.log('val_kl_loss', kl_loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_recon_loss', recon_loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "\n",
    "\n",
    "        return x_out, loss\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        ### WIP\n",
    "        x = batch\n",
    "        mu, log_var, x_out, hidden = self.forward(x)\n",
    "\n",
    "        # Loss\n",
    "        kl_loss = (-0.5*(1+log_var - mu**2 -\n",
    "                         torch.exp(log_var)).sum(dim=1)).mean(dim=0)\n",
    "        recon_loss_criterion = nn.MSELoss()\n",
    "        recon_loss = recon_loss_criterion(x, x_out)\n",
    "        loss = recon_loss*self.alpha + kl_loss\n",
    "\n",
    "        return  mu, log_var, x_out, hidden\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    # Functions for dataloading\n",
    "    def train_dataloader(self):\n",
    "        train_set = _dataset(self.dataset, category=\"train\")\n",
    "        return DataLoader(train_set, batch_size=self.batch_size, num_workers=12)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_set = _dataset(self.dataset, category=\"validation\")\n",
    "        return DataLoader(val_set, batch_size=self.batch_size, num_workers=12)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        val_set = _dataset(self.dataset, category=\"test\")\n",
    "        return DataLoader(val_set, batch_size=self.batch_size, num_workers=12)\n"
   ]
  }
 ]
}