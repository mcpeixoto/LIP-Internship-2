{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning import Trainer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from typing import Optional\n",
    "from config import processed_data_path\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.notebook import tqdm\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from scipy.stats import wasserstein_distance \n",
    "import joblib\n",
    "import optuna\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import wasserstein_distance\n",
    "import threading\n",
    "import concurrent\n",
    "from sklearn.metrics import r2_score\n",
    "from VAE_OPTIMIZATION_WD import _dataset, VAE, study, compare_continuous, compare_integer#, compare_distributions_binned_aux, compare_distributions_binned"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the Study"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "print(\" TRIAL NUMBER:\", trial.number)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optuna Graphs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optuna.visualization.plot_param_importances(study) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "### Load the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Nao é o 49, 53, 33, 26, 25, 23, 22, 16\n",
    "#name = \"wd-sample_vs_data_trial_16\"\n",
    "#name = \"wd-sample_vs_data_trial_{}\".format(trial.number)\n",
    "name = \"CustomTrain_WD-Data_vs_Sampling_trial_51\"\n",
    "model = VAE.load_from_checkpoint(\n",
    "    #join('models', f\"sample_vs_data_trial_{study.best_trial.number}.ckpt\"),\n",
    "    join(\"models\", name + \".ckpt\"),\n",
    "    #trial = study.trials[16], \n",
    "    trial = optuna.trial.FixedTrial(study.best_trial.params), \n",
    "    dataset = \"bkg\", \n",
    "    batch_size=512)\n",
    "\n",
    "model.eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Bkg\n",
    "bkg = _dataset(category='test',variant='bkg').all_data()\n",
    "bkg_name = bkg['name']\n",
    "bkg_weights = bkg['weights']\n",
    "bkg.drop(columns=['weights', 'name'], inplace=True)\n",
    "bkg.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check if z ~ N(0,1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_, _, x_out, hidden = model.test_step(torch.from_numpy(bkg.to_numpy(dtype=np.float32)))\n",
    "x_out = x_out.detach().numpy()\n",
    "hidden = hidden.detach().numpy()\n",
    "hidden.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=[20,15])\n",
    "i = 0\n",
    "\n",
    "axes = fig.add_subplot(1,2,i+1)\n",
    "i += 1\n",
    "axes.matshow(pd.DataFrame(hidden).corr().apply(abs))\n",
    "\n",
    "axes = fig.add_subplot(1,2,i+1)\n",
    "i += 1\n",
    "axes.matshow(pd.DataFrame(hidden).corr().apply(abs).apply(np.log))\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corr = pd.DataFrame(hidden).corr().apply(abs)\n",
    "corr.replace(1, 0, inplace=True)\n",
    "\n",
    "print(\"Max:\\t\", round(corr.max().max(), 5))\n",
    "print(\"Mean:\\t\", round(corr.mean().mean(), 5))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "fig = plt.figure(figsize=[25,70])\n",
    "i = 0\n",
    "\n",
    "total_mean = []\n",
    "total_std = []\n",
    "for x in range(hidden.shape[1]):\n",
    "    axes = fig.add_subplot(20,4,i+1)\n",
    "    i += 1\n",
    "\n",
    "    axes.hist(hidden[:, x], bins='auto')\n",
    "    axes.axis(xmin=-5,xmax=5)\n",
    "    #axes.title(f\"Z{x}\")\n",
    "    total_mean.append(hidden[:, x].mean())\n",
    "    total_std.append(hidden[:, x].std())\n",
    "    print(\"Mean:\", hidden[:, x].mean(), \"\\tStd:\", hidden[:, x].std())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Total average mean\", np.array(total_mean).mean())\n",
    "print(\"Total average std\", np.array(total_std).mean())\n",
    "fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bkg Data vs Random Sampling Decoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random sample from N(0,1)\n",
    "sample = model.decode(torch.rand(bkg.shape[0], study.best_trial.params['hidden_size'])).detach().numpy()\n",
    "\n",
    "# Make it a dataframe\n",
    "sample = pd.DataFrame(sample, columns=bkg.columns)\n",
    "sample.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_graphs2(background, signal, bins=50, num_cols=4, first_name=\"Signal\", second_name=\"Sampled\"):\n",
    "    WD_SCORE = 0\n",
    "    R2_SCORE = 0\n",
    "    # Ignoring irrelevant features such as 'name' and 'weights' in\n",
    "    # the plotting of the data\n",
    "    features  = list(background.columns)\n",
    "    for x in ['name', 'weights']: \n",
    "        try:\n",
    "            features.remove(x)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Plot creation\n",
    "    num_rows = int(np.ceil((len(list(background.columns)) - 1) / num_cols)) +1\n",
    "    fig, ax = plt.subplots(num_rows, num_cols, figsize=(40, 60))\n",
    "    i= 0\n",
    "\n",
    "    for x in tqdm(features, total=len(features), desc=\"Processing...\"):\n",
    "\n",
    "        # Plot  \n",
    "        row, col = int(i/num_cols), i%num_cols\n",
    "        #print(row, col, i)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "        # Define histogram range\n",
    "        hist_min = min(signal[x].min(), background[x].min())\n",
    "        hist_max = max(signal[x].max(), background[x].max())\n",
    "        hist_range = (hist_min, hist_max)\n",
    "\n",
    "\n",
    "        ax[row, col].set_title(x)\n",
    "        ax[row, col].set_yscale('log')\n",
    "        \n",
    "        ax[row, col].hist(background[x], bins=bins, alpha=0.5, label=first_name, range=hist_range)\n",
    "        ax[row, col].hist(signal[x], bins=bins, alpha=0.5, label=second_name,  range=hist_range)\n",
    "        \n",
    "        ax[row, col].autoscale(enable=True) \n",
    "        ax[row, col].legend()\n",
    "\n",
    "        #WD_SCORE += wasserstein_distance(background[x], signal[x])\n",
    "        #R2_SCORE += r2_score(background[x],signal[x])\n",
    "        \n",
    "\n",
    "    fig.tight_layout()\n",
    "    #plt.savefig('1_explore_data.png', bbox_inches='tight', dpi=100)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_graphs2(bkg, sample)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "objective_score = 0\n",
    "for idx in range(bkg.shape[1]):\n",
    "    feature = bkg.columns[idx]\n",
    "    if \"Tag\" in feature or \"Multi\" in feature:\n",
    "        #print(\"Feature\", feature, \"é inteiro\")\n",
    "        objective_score += compare_integer(bkg.to_numpy()[:, idx], bkg_weights.to_numpy(), sample.to_numpy()[:, idx], np.ones(bkg_weights.shape))\n",
    "    else:\n",
    "        #print(\"Feature\", feature, \"é continuo\")\n",
    "        objective_score += compare_continuous(bkg.to_numpy()[:, idx], bkg_weights.to_numpy(), sample.to_numpy()[:, idx], np.ones(bkg_weights.shape))\n",
    "print(\"WD Score:\", objective_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bkg Data vs Reconstruction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_graphs3(first, second, first_name=\"Signal\", second_name=\"Sampled\", bins=50, num_cols=4, num_features=69):\n",
    "\n",
    "    R2_SCORE = 0\n",
    "\n",
    "    # Reshape\n",
    "    first = first.reshape(num_features, -1)\n",
    "    second = second.reshape(num_features, -1)\n",
    "\n",
    "    # Plot creation\n",
    "    num_rows = int(np.ceil((num_features - 1) / num_cols)) +1\n",
    "    fig, ax = plt.subplots(num_rows, num_cols, figsize=(40, 60))\n",
    "    i= 0\n",
    "\n",
    "    for x in tqdm(range(num_features), total=num_features, desc=\"Processing...\"):\n",
    "\n",
    "        # Plot  \n",
    "        row, col = int(i/num_cols), i%num_cols\n",
    "        #print(row, col, i)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "        # Define histogram range\n",
    "        hist_min = min(first[x].min(), second[x].min())\n",
    "        hist_max = max(first[x].max(), second[x].max())\n",
    "        hist_range = (hist_min, hist_max)\n",
    "\n",
    "\n",
    "        ax[row, col].set_title(x)\n",
    "        ax[row, col].set_yscale('log')\n",
    "        \n",
    "        ax[row, col].hist(first[x], bins=bins, alpha=0.5, label=first_name, range=hist_range)\n",
    "        ax[row, col].hist(second[x], bins=bins, alpha=0.5, label=second_name,  range=hist_range)\n",
    "\n",
    "        x = x.cpu().numpy()\n",
    "        output = output.cpu().numpy()\n",
    "\n",
    "        #print(\"Input\", np.isnan(x).any())\n",
    "        #print(\"Output\", np.isnan(output).any())\n",
    "\n",
    "\n",
    "        #R2_SCORE += r2_score(first[x],second[x])\n",
    "        \n",
    "\n",
    "    fig.tight_layout()\n",
    "    #plt.savefig('1_explore_data.png', bbox_inches='tight', dpi=100)\n",
    "    plt.show()\n",
    "    #print(\"R2_SCORE:\", R2_SCORE/num_features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_graphs2(bkg, pd.DataFrame(x_out, columns=bkg.columns), first_name=\"Bkg\", second_name=\"Reconstruction\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"R2 Score:\", r2_score(bkg.to_numpy(), x_out, sample_weight=bkg_weights.to_numpy()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot error distributions and ROC Scores"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bkg = _dataset(category='test',variant='bkg').all_data()\n",
    "signal = _dataset(category='all',variant='signal').all_data()\n",
    "# signal.drop(columns=['weights'], inplace=True)\n",
    "\n",
    "data = pd.concat([signal, bkg])\n",
    "del signal, bkg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix,precision_score\n",
    "\n",
    "def plot_graphs3(data, bins=50, num_cols=4):\n",
    "    # Plot creation\n",
    "    features = list(data['name'].unique())\n",
    "\n",
    "    distributions = {}\n",
    "\n",
    "    for x in tqdm(features, total=len(features), desc=\"Processing...\"):\n",
    "\n",
    "        ## Get the relevant data\n",
    "        sData = data.loc[data['name'] == x].drop(columns=['name', 'weights'])\n",
    "        sData = torch.from_numpy(\n",
    "            sData.to_numpy(dtype=np.float32)\n",
    "        )\n",
    "\n",
    "        ## Pass input through model to get the output\n",
    "        _, _, output, _ = model.test_step(sData)\n",
    "\n",
    "        ## Calculate the error dist\n",
    "        error_dist = (sData - output)**2\n",
    "        del output, sData\n",
    "        error_dist = error_dist.detach().numpy().sum(axis=1)\n",
    "        error_dist = np.log(error_dist)\n",
    "        # Append to list\n",
    "        distributions[x] = error_dist\n",
    "\n",
    "    ## Normalize the distributions\n",
    "    # This way all values should be between 0 and 1\n",
    "\n",
    "    # x transform\n",
    "    min_of_dist = min(map(lambda x: min(x), distributions.values()))\n",
    "    for x in distributions:\n",
    "        distributions[x] = distributions[x] - min_of_dist\n",
    "    # scale\n",
    "    max_of_dist = max(map(lambda x: max(x), distributions.values()))\n",
    "    for x in distributions:\n",
    "        distributions[x] = distributions[x] / max_of_dist\n",
    "    \n",
    "    ## Plot error distributions\n",
    "    fig, ax = plt.subplots( figsize=(10,10))\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    for x in tqdm(distributions.keys(), desc=\"Processing...\"):\n",
    "\n",
    "        hist_range = (0, 1)        \n",
    "\n",
    "        if x != 'bkg.h5':\n",
    "            ax.hist(distributions[x], bins=bins, alpha=0.9, label=x.replace(\".h5\", \"\"), range=hist_range, histtype=u'step', linewidth=2, density=True)\n",
    "        else:\n",
    "            ax.hist(distributions[x], bins=bins, alpha=0.2, label=x.replace(\".h5\", \"\"), range=hist_range, density=True)\n",
    "\n",
    "    ax.autoscale(enable=True) \n",
    "    ax.set_title(\"Error Dist.\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "    ## Plot ROC Curves\n",
    "    fig_roc, ax_roc = plt.subplots(figsize=(10,10))\n",
    "    for x in tqdm(distributions.keys(), desc=\"Processing...\"):\n",
    "        if x != 'bkg.h5':\n",
    "            # Set labels\n",
    "            bkg_labels = np.zeros(distributions['bkg.h5'].shape[0]).astype(int)\n",
    "            signal_labels = np.ones(distributions[x].shape[0]).astype(int)\n",
    "            labels = np.concatenate([bkg_labels, signal_labels])\n",
    "\n",
    "            # Set Scores\n",
    "            score = np.concatenate([distributions['bkg.h5'], distributions[x]]) \n",
    "\n",
    "            # Set weights\n",
    "            weights = pd.concat([\n",
    "                                data[data['name'] == \"bkg.h5\"]['weights'], \n",
    "                                data[data['name'] == x]['weights']\n",
    "                                ])\n",
    "\n",
    "            # Get Curve\n",
    "            fpr, tpr, thr = roc_curve(\n",
    "                    y_true=labels, \n",
    "                    y_score=score,\n",
    "                    sample_weight=weights\n",
    "                    )\n",
    "            \n",
    "            \n",
    "            ax_roc.plot(fpr, tpr, label=x.replace('.h5', ''))\n",
    "            ax_roc.plot([0,1],[0,1], 'k--')\n",
    "   \n",
    "            print(f\"ROC SCORE for {x.replace('.h5', '')}:\", \n",
    "                    roc_auc_score(y_true=labels, \n",
    "                    y_score=score,\n",
    "                    sample_weight=weights))\n",
    "            #print(score.min(), score.max())\n",
    "            #print(np.unique(np.rint(score)))\n",
    "            print(\"Confusion matrix:\")\n",
    "            print(confusion_matrix(labels, np.rint(score)))\n",
    "            print(\"\\n\")\n",
    "    fig_roc.show()\n",
    "    ax_roc.set_title(f\"BKG vs Signals\")\n",
    "    ax_roc.legend()\n",
    "                \n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_graphs3(data)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}