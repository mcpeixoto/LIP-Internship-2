{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('env_tensorflow': venv)"
  },
  "interpreter": {
   "hash": "7cfbf909c009492aafe796b3bfb5ce0117a930ec56b9e9020f69c22f95d83cb1"
  },
  "metadata": {
   "interpreter": {
    "hash": "00d0da2beda986aec30816e4cd7fb4d22aa303e0c7be1e29f09be47940c81e46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import layers\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from config import processed_data_path"
   ]
  },
  {
   "source": [
    "### Data\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(variant, category, random_seed=42):\n",
    "    # variant -> 'VLQ_HG', 'VLQ_SEM_HG', 'bkg', 'FCNC'\n",
    "    # category -> train, validation, test\n",
    "    \n",
    "    # TODO: Improve efficiency/handle names\n",
    "    \n",
    "    # Check if variant is valid\n",
    "    assert variant in {'VLQ_HG', 'VLQ_SEM_HG', 'bkg', 'FCNC'}, \"Invalid variant!\"\n",
    "\n",
    "    # With specified variant, get data\n",
    "    file = join(processed_data_path, variant+\".csv\")\n",
    "    data = pd.read_csv(file, index_col=0)\n",
    "\n",
    "    # Shuffle the dataframe\n",
    "    data = data.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "\n",
    "    # This will equally devide the dataset into \n",
    "    # train, validation and test\n",
    "    train, validation, test = np.split(data.sample(frac=1), [int(len(data)*(1/3)), int(len(data)*(2/3))])\n",
    "    \n",
    "    if category == \"train\":\n",
    "        data = train\n",
    "    elif category == \"validation\":\n",
    "        data = validation\n",
    "    elif category == \"test\":\n",
    "        data = test\n",
    "    \n",
    "    # This data we want on a seperate variable\n",
    "    weights = data[\"weights\"].to_numpy(dtype=np.float16)\n",
    "    name = data[\"name\"]\n",
    "\n",
    "    data.drop([\"name\", \"weights\"], axis=1, inplace=True)\n",
    "\n",
    "    data = data.to_numpy(dtype=np.float16)\n",
    "    return data, weights, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "x_train, x_train_weights, _ = data(category='train',\n",
    "                                    variant='bkg')\n",
    "x_val, _, _ = data(category='validation',\n",
    "                                variant='bkg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reparametrize(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"encoder\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            [(None, 69)]         0                                            \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 128)          8960        input_3[0][0]                    \n__________________________________________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)       (None, 128)          0           dense_5[0][0]                    \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 2)            258         leaky_re_lu_3[0][0]              \n__________________________________________________________________________________________________\ndense_7 (Dense)                 (None, 2)            258         leaky_re_lu_3[0][0]              \n__________________________________________________________________________________________________\nreparametrize_1 (Reparametrize) (None, 2)            0           dense_6[0][0]                    \n                                                                 dense_7[0][0]                    \n==================================================================================================\nTotal params: 9,476\nTrainable params: 9,476\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 2\n",
    "\n",
    "## Encoder\n",
    "inputs = layers.Input(shape=(69,))\n",
    "z = layers.Dense(128)(inputs)\n",
    "z = layers.LeakyReLU()(z)\n",
    "\n",
    "mean = layers.Dense(hidden_size)(z)\n",
    "log_var = layers.Dense(hidden_size)(z)                \n",
    "latent = Reparametrize()([mean, log_var])\n",
    "\n",
    "encoder = keras.Model(inputs=[inputs], outputs=[mean, log_var, latent], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"decoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 2)]               0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 128)               384       \n_________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 69)                8901      \n_________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)    (None, 69)                0         \n=================================================================\nTotal params: 9,285\nTrainable params: 9,285\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Decoder \n",
    "decoder_inputs = layers.Input(shape=[hidden_size])\n",
    "z = layers.Dense(128)(decoder_inputs)\n",
    "z = layers.LeakyReLU()(z)\n",
    "z = layers.Dense(69)(z)\n",
    "outputs = layers.LeakyReLU()(z)\n",
    "\n",
    "decoder = keras.Model(inputs=[decoder_inputs], outputs=[outputs], name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "## Joining everything\n",
    "#_, _, latent = encoder(inputs)\n",
    "#reconstruction = decoder(latent)\n",
    "#model = keras.Model(inputs=[inputs], outputs=[reconstruction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        # Train\n",
    "        self.total_train_loss_tracker = keras.metrics.Mean(name=\"total_train_loss\")\n",
    "        self.recon_train_loss_tracker = keras.metrics.Mean(name=\"recon_train_loss\")\n",
    "        self.kl_train_loss_tracker = keras.metrics.Mean(name=\"kl_train_loss\")\n",
    "        # Val\n",
    "        self.total_val_loss_tracker = keras.metrics.Mean(name=\"total_val_loss\")\n",
    "        self.recon_val_loss_tracker = keras.metrics.Mean(name=\"recon_val_loss\")\n",
    "        self.kl_val_loss_tracker = keras.metrics.Mean(name=\"kl_val_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_train_loss_tracker,\n",
    "            self.recon_train_loss_tracker,\n",
    "            self.kl_train_loss_tracker,\n",
    "            self.total_val_loss_tracker,\n",
    "            self.recon_val_loss_tracker,\n",
    "            self.kl_val_loss_tracker\n",
    "        ]\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            x, y, weights = data\n",
    "\n",
    "            z_mean, z_log_var, z = self.encoder(x)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            ## Loss\n",
    "            # reconstruction\n",
    "            recon_loss = binary_crossentropy(x, reconstruction) # Shape = BATCH_SIZE\n",
    "            # Weights on recon loss\n",
    "            recon_train_loss = (weights * recon_loss) / K.sum(weights)\n",
    "            recon_loss = K.mean(recon_loss, axis = 0)\n",
    "\n",
    "            # KL\n",
    "            kl_loss = -0.5 * (1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n",
    "            kl_loss = K.mean(K.sum(kl_loss, axis=1), axis=0)\n",
    "            # Weights on KL Loss\n",
    "            kl_loss = (weights * kl_loss) / K.sum(weights)\n",
    "            kl_loss = K.mean(kl_loss, axis = 0)\n",
    "\n",
    "            # Total\n",
    "            total_loss = recon_loss + kl_loss\n",
    "\n",
    "        # Step\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        # Log\n",
    "        self.total_train_loss_tracker.update_state(total_loss)\n",
    "        self.recon_train_loss_tracker.update_state(recon_loss)\n",
    "        self.kl_train_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            \"total_train_loss\": self.total_train_loss_tracker.result(),\n",
    "            \"recon_train_loss\": self.recon_train_loss_tracker.result(),\n",
    "            \"kl_train_loss\": self.kl_train_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def validation_step(self, data):\n",
    "        # No tape, we don't need gradients\n",
    "        x, y = data\n",
    "\n",
    "        print(\"1\", type(x), type(y))\n",
    "\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        reconstruction = self.decoder(z)\n",
    "\n",
    "        ## Loss\n",
    "        # reconstruction\n",
    "        recon_loss = binary_crossentropy(x, reconstruction) # Shape = BATCH_SIZE\n",
    "\n",
    "        # KL\n",
    "        kl_loss = -0.5 * (1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n",
    "        kl_loss = K.mean(K.sum(kl_loss, axis=1), axis=0)\n",
    "\n",
    "        # Total\n",
    "        total_loss = recon_loss + kl_loss\n",
    "\n",
    "\n",
    "        loss = recon_loss*self.alpha + kl_loss\n",
    "\n",
    "        # Log\n",
    "        self.total_val_loss_tracker.update_state(total_loss)\n",
    "        self.recon_val_loss_tracker.update_state(recon_loss)\n",
    "        self.kl_val_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            \"total_val_loss\": self.total_val_loss_tracker.result(),\n",
    "            \"recon_val_loss\": self.recon_val_loss_tracker.result(),\n",
    "            \"kl_val_loss\": self.kl_val_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # No tape, we don't need gradients\n",
    "        x, y = data\n",
    "\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction, z_mean, z_log_var, z # z is the latent vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Callbacks\n",
    "# Model name\n",
    "name = str(datetime.now().strftime(\"%d_%m_%Y__%H_%M_%S\"))\n",
    "\n",
    "# Tensorboard\n",
    "TB = keras.callbacks.TensorBoard(log_dir=join(\"logs\", name), write_images=True)\n",
    "\n",
    "# Early Stopping\n",
    "ES = keras.callbacks.EarlyStopping(monitor=\"total_val_loss\", patience=30, verbose=2, mode=\"min\")\n",
    "\n",
    "# Model Checkpoint\n",
    "MC = keras.callbacks.ModelCheckpoint(filepath=join(\"models_tf\", name), save_best_only=True, monitor=\"total_val_loss\", mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "90/92 [============================>.] - ETA: 0s - total_train_loss: -0.4637 - recon_train_loss: -0.4832 - kl_train_loss: 0.0194"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'items'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b3ea8a6b1b64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = vae.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/D/estagio_lip_2/env_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1214\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1215\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/D/estagio_lip_2/env_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/D/estagio_lip_2/env_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_test_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/D/estagio_lip_2/env_tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_test_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2392\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msummary_ops_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_if\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_val_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2393\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2394\u001b[0m           summary_ops_v2.scalar(\n\u001b[1;32m   2395\u001b[0m               \u001b[0;34m'evaluation_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_vs_iterations'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# Fit\n",
    "history = vae.fit(\n",
    "    # Train\n",
    "    x=x_train, \n",
    "    y=x_train,\n",
    "    sample_weight=x_train_weights,\n",
    "\n",
    "    # Validation\n",
    "    validation_data=(x_val,x_val),\n",
    "\n",
    "    # Hyper-parameters    \n",
    "    epochs=30, \n",
    "    batch_size=4048,\n",
    "    callbacks=[TB, ES, MC])"
   ]
  },
  {
   "source": [
    "### Ploting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d02bb1c3e567>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                     variant='bkg')\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mplot_label_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_label_clusters(vae, data, labels):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = vae.encoder.predict(data)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "x_test, _, x_test_names = data(category='test',\n",
    "                                    variant='bkg')\n",
    "\n",
    "plot_label_clusters(vae, x_train, y_train)"
   ]
  }
 ]
}