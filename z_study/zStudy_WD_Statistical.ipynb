{
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning import Trainer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from typing import Optional\n",
    "from config import processed_data_path\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.notebook import tqdm\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from scipy.stats import wasserstein_distance \n",
    "import joblib\n",
    "import optuna\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import wasserstein_distance\n",
    "import threading\n",
    "import concurrent\n",
    "from sklearn.metrics import r2_score\n",
    "from zStudy_WD import VAE, _dataset, compare_integer, compare_continuous\n",
    "from os.path import join, basename, exists\n",
    "from os import getcwd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix,precision_score\n",
    "import pickle\n",
    "from math import log10, floor\n",
    "import gc\n",
    "import sqlite3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# For saving img\n",
    "dir_name = basename(getcwd())\n",
    "img_dir = join(getcwd(), \"images\")\n",
    "if not exists(img_dir):\n",
    "    os.mkdir(img_dir)\n",
    "variante= 'WD'\n",
    "dir_name"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'z_study'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def round_sig(x, sig=2):\n",
    "    # rounds to significant digits\n",
    "   return round(x, sig-int(floor(log10(abs(x))))-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the Study"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Load bkg data\n",
    "bkg = _dataset(category='validation',variant='bkg').all_data() # The data category is validation because I accidentally used 'test' on the validation step while training\n",
    "#bkg_name = bkg['name']\n",
    "bkg_weights = bkg['weights']\n",
    "bkg.drop(columns=['weights', 'name'], inplace=True)\n",
    "\n",
    "# Get bkg shapes etc\n",
    "bkg_shape = bkg.shape\n",
    "bkg_columns = bkg.columns\n",
    "bkg = bkg.to_numpy(dtype=np.float32)\n",
    "bkg_weights = bkg_weights.to_numpy(dtype=np.float16)\n",
    "\n",
    "# Get database names\n",
    "con = sqlite3.connect(\"optimization.db\")\n",
    "cursor = con.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM studies\")\n",
    "names = [x[1] for x in cursor.fetchall() if variante in x[1]]\n",
    "names = sorted(names, key=lambda x: int(x.split(\"-\")[1].replace(' zdim ', '').replace(' ', '')))\n",
    "del con, cursor\n",
    "gc.collect()\n",
    "\n",
    "# Define variablers\n",
    "book = {}\n",
    "book[\"x_axis\"] = []\n",
    "book[\"r2\"] = []\n",
    "book[\"wd\"] = []\n",
    "book[\"means\"] = []\n",
    "book[\"stds\"] = []\n",
    "book[\"correlations\"] = []\n",
    "\n",
    "# Let's have 10 samples of each data point\n",
    "num_runs = 10 \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Load bkg data\n",
    "bkg = _dataset(category='validation',variant='bkg').all_data() # The data category is validation because I accidentally used 'test' on the validation step while training\n",
    "#bkg_name = bkg['name']\n",
    "bkg_weights = bkg['weights']\n",
    "bkg.drop(columns=['weights', 'name'], inplace=True)\n",
    "\n",
    "# Get bkg shapes etc\n",
    "bkg_shape = bkg.shape\n",
    "bkg_columns = bkg.columns\n",
    "bkg = bkg.to_numpy(dtype=np.float32)\n",
    "bkg_weights = bkg_weights.to_numpy(dtype=np.float16)\n",
    "\n",
    "# Get database names\n",
    "con = sqlite3.connect(\"optimization.db\")\n",
    "cursor = con.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM studies\")\n",
    "names = [x[1] for x in cursor.fetchall() if variante in x[1]]\n",
    "names = sorted(names, key=lambda x: int(x.split(\"-\")[1].replace(' zdim ', '').replace(' ', '')))\n",
    "del con, cursor\n",
    "gc.collect()\n",
    "\n",
    "# Define variablers\n",
    "book = {}\n",
    "book[\"x_axis\"] = []\n",
    "book[\"r2\"] = []\n",
    "book[\"wd\"] = []\n",
    "book[\"means\"] = []\n",
    "book[\"stds\"] = []\n",
    "book[\"correlations\"] = []\n",
    "\n",
    "# Let's have 10 samples of each data point\n",
    "num_runs = 10 \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Means, stds, r2s, wds"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for n in tqdm(range(num_runs)):\n",
    "    book[\"means\"].append([])\n",
    "    book[\"stds\"].append([])\n",
    "    book[\"wd\"].append([])\n",
    "    book[\"r2\"].append([])\n",
    "    book[\"correlations\"].append([])\n",
    "    for i,name in tqdm(enumerate(names), total=len(names)):\n",
    "        zdim = int(name.split(\"-\")[1].replace(' zdim ', '').replace(' ', ''))\n",
    "        #print(\"Zdim:\", zdim)\n",
    "\n",
    "        study = optuna.load_study(study_name=name, storage=\"sqlite:///optimization.db\")\n",
    "        \n",
    "        trial = study.best_trial\n",
    "        \n",
    "\n",
    "        # load model\n",
    "        model = VAE.load_from_checkpoint(\n",
    "            join('models', f\"{variante.lower()}_zStudy-zdim_{zdim}_trial_{trial.number}.ckpt\"),\n",
    "            trial = optuna.trial.FixedTrial(trial.params), \n",
    "            zdim = zdim,\n",
    "            dataset = \"bkg\", \n",
    "            batch_size=512)\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "\n",
    "        ## Sample from the model\n",
    "        # Random sample from N(0,1)\n",
    "        sample = model.decode(torch.rand(bkg_shape[0], zdim)).detach().numpy()\n",
    "\n",
    "        # Calculate WD Score\n",
    "        objective_score = 0\n",
    "        for idx in range(bkg_shape[1]):\n",
    "            feature = bkg_columns[idx]\n",
    "            if \"Tag\" in feature or \"Multi\" in feature:\n",
    "                #print(\"Feature\", feature, \"é inteiro\")\n",
    "                objective_score += compare_integer(bkg[:, idx], bkg_weights, sample[:, idx], np.ones(bkg_weights.shape))\n",
    "            else:\n",
    "                #print(\"Feature\", feature, \"é continuo\")\n",
    "                objective_score += compare_continuous(bkg[:, idx], bkg_weights, sample[:, idx], np.ones(bkg_weights.shape))\n",
    "\n",
    "        del sample, study, trial\n",
    "        gc.collect()\n",
    "\n",
    "        # Calculate R2 Score\n",
    "        x_out, hidden = model.test_step(torch.from_numpy(bkg))\n",
    "        hidden = hidden.detach().numpy()\n",
    "        x_out = x_out.detach().numpy()\n",
    "        r2_score_ = r2_score(bkg, x_out, sample_weight=bkg_weights)\n",
    "        del x_out\n",
    "        gc.collect()\n",
    "\n",
    "        # Calculate hidden mean\n",
    "        total_mean = []\n",
    "        total_std = []\n",
    "        for x in range(hidden.shape[1]):\n",
    "            total_mean.append(hidden[:, x].mean())\n",
    "            total_std.append(hidden[:, x].std())\n",
    "        mean = np.array(total_mean).mean()\n",
    "        std = np.array(total_std).mean()\n",
    "\n",
    "        # Correlations\n",
    "        corr = pd.DataFrame(hidden).corr().apply(abs)\n",
    "        corr.replace(1, 0, inplace=True)        \n",
    "        \n",
    "        # Storing values\n",
    "\n",
    "        book[\"means\"][-1].append(round(abs(mean),6))\n",
    "        book[\"stds\"][-1].append(round(std,6))\n",
    "        book[\"wd\"][-1].append(round(objective_score,6))\n",
    "        book[\"r2\"][-1].append(round(r2_score_,6))\n",
    "        book[\"correlations\"][-1].append(round(corr.mean().mean(), 5))\n",
    "\n",
    "        del hidden, model, mean, std\n",
    "        gc.collect()\n",
    "\n",
    "del bkg, bkg_columns, bkg_shape, bkg_weights\n",
    "gc.collect()\n",
    "\n",
    "# Save book\n",
    "pickle.dump( book, open(variante+\"_book.p\", \"wb\" ) )"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d159cf41f694ecba964a79e3fd27d7a"
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75f9455570e04b85928d8b27908931f9"
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rocs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bkg = _dataset(category='validation',variant='bkg').all_data() # The data category is validation because I accidentally used 'test' on the validation step while training\n",
    "signal = _dataset(category='all',variant='signal').all_data()\n",
    "\n",
    "data = pd.concat([signal, bkg])\n",
    "del signal, bkg\n",
    "\n",
    "\n",
    "rocs = {}\n",
    "\n",
    "for n in tqdm(range(num_runs)):\n",
    "    features = list(data['name'].unique())\n",
    "    distributions = {}\n",
    "\n",
    "    for name in features:\n",
    "        name = name.replace('.h5', '')\n",
    "        if name not in rocs.keys():\n",
    "            rocs[name] = []\n",
    "        rocs[name].append([])\n",
    "\n",
    "    for i,name in tqdm(enumerate(names), total=len(names)):\n",
    "        zdim = int(name.split(\"-\")[1].replace(' zdim ', '').replace(' ', ''))\n",
    "\n",
    "        study = optuna.load_study(study_name=name, storage=\"sqlite:///optimization.db\")\n",
    "        trial = study.best_trial\n",
    "        model = VAE.load_from_checkpoint(\n",
    "            join('models', f\"{variante.lower()}_zStudy-zdim_{zdim}_trial_{trial.number}.ckpt\"),\n",
    "            trial = optuna.trial.FixedTrial(trial.params), \n",
    "            zdim = zdim,\n",
    "            dataset = \"bkg\", \n",
    "            batch_size=512)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for x in features:\n",
    "\n",
    "            ## Get the relevant data\n",
    "            sData = data.loc[data['name'] == x].drop(columns=['name', 'weights'])\n",
    "            sData = torch.from_numpy(\n",
    "                sData.to_numpy(dtype=np.float32)\n",
    "            )\n",
    "\n",
    "            ## Pass input through model to get the output\n",
    "            output, _ = model.test_step(sData)\n",
    "\n",
    "            ## Calculate the error dist\n",
    "            error_dist = (sData - output)**2\n",
    "            del output, sData\n",
    "            gc.collect()\n",
    "            error_dist = error_dist.detach().numpy().sum(axis=1)\n",
    "            error_dist = np.log(error_dist)\n",
    "            # Append to list\n",
    "            distributions[x] = error_dist\n",
    "\n",
    "        del model, trial, study, error_dist\n",
    "        gc.collect()\n",
    "\n",
    "        ## Normalize the distributions\n",
    "        # This way all values should be between 0 and 1\n",
    "        # x transform\n",
    "        min_of_dist = min(map(lambda x: min(x), distributions.values()))\n",
    "        for x in distributions:\n",
    "            distributions[x] = distributions[x] - min_of_dist\n",
    "        # scale\n",
    "        max_of_dist = max(map(lambda x: max(x), distributions.values()))\n",
    "        for x in distributions:\n",
    "            distributions[x] = distributions[x] / max_of_dist\n",
    "\n",
    "        for x in distributions.keys():\n",
    "            if x != 'bkg.h5':\n",
    "                # Set labels\n",
    "                bkg_labels = np.zeros(distributions['bkg.h5'].shape[0]).astype(int)\n",
    "                signal_labels = np.ones(distributions[x].shape[0]).astype(int)\n",
    "                labels = np.concatenate([bkg_labels, signal_labels])\n",
    "\n",
    "                # Set Scores\n",
    "                score = np.concatenate([distributions['bkg.h5'], distributions[x]]) \n",
    "\n",
    "                # Set weights\n",
    "                weights = pd.concat([\n",
    "                                    data[data['name'] == \"bkg.h5\"]['weights'], \n",
    "                                    data[data['name'] == x]['weights']\n",
    "                                    ])\n",
    "\n",
    "                name = x.replace('.h5', '')\n",
    "                if name not in rocs.keys():\n",
    "                    rocs[name] = []\n",
    "\n",
    "                \n",
    "                rocs[name][-1].append(roc_auc_score(y_true=labels, y_score=score,sample_weight=weights))\n",
    "\n",
    "del distributions, weights\n",
    "gc.collect()\n",
    "# Save rocs\n",
    "pickle.dump( rocs, open(variante+\"_rocs.p\", \"wb\" ) )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load pickles\n",
    "book = pickle.load( open( variante+\"_book.p\", \"rb\" ) )\n",
    "rocs = pickle.load( open( variante+\"_rocs.p\", \"rb\" ))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ignore = []\n",
    "for x in wd:\n",
    "    ignore.append(x[0])\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "book[\"x_axis\"] = [x for x in range(5, 105, 5)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# PLOT R2\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('zdim')\n",
    "ax1.set_ylabel('R2 (more is better)', color=color)\n",
    "r2 = np.array(book['r2'])\n",
    "r2_y = np.mean(r2, axis=0)\n",
    "#ax1.plot(book[\"x_axis\"], np.mean(r2, axis=0), color=color)\n",
    "ax1.errorbar(book[\"x_axis\"], r2_y, yerr=(np.max(r2, axis=0)-r2_y, r2_y-np.min(r2, axis=0)), ecolor=\"black\", color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# PLOT WD\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('-WD (more is better)', color=color)  # we already handled the x-label with ax1\n",
    "wd = np.array(book['wd'])\n",
    "wd_y = np.mean(wd, axis=0)\n",
    "#ax2.plot(book[\"x_axis\"], -np.mean(wd, axis=0), color=color)\n",
    "ax2.errorbar(book[\"x_axis\"], -wd_y, yerr=(np.max(wd, axis=0)-wd_y, wd_y-np.min(wd, axis=0)), ecolor=\"black\", color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.annotate(f'R2: {str(round_sig(r2.max(),5))} (best)', (0.5,-0.2), xycoords='axes fraction', textcoords='offset points', ha='center')\n",
    "plt.annotate(f'WD: {str(round_sig(wd.min(),5))} (best)', (0.5,-0.275), xycoords='axes fraction', textcoords='offset points', ha='center')\n",
    "plt.axvline(47, ymin=0, ymax=1, color=\"black\", linestyle='--', label=\"Nº of features \\nin the data\")\n",
    "plt.legend(loc='best')\n",
    "fig.savefig(join(img_dir, dir_name+variante+\"_r2_wd.png\"), bbox_inches = 'tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('zdim')\n",
    "ax1.set_ylabel('abs(Mean) (less is better)', color=color)\n",
    "#ax1.plot(book[\"x_axis\"], means, color=color)\n",
    "means = np.array(book['means'])\n",
    "means_y = np.mean(means, axis=0)\n",
    "ax1.errorbar(book[\"x_axis\"], means_y, yerr=(np.max(means, axis=0)-means_y, means_y-np.min(means, axis=0)), ecolor=\"black\", color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('abs(Std-1) (less is better)', color=color)  # we already handled the x-label with ax1\n",
    "#ax2.plot(book[\"x_axis\"], [abs(x-1) for x in stds], color=color)\n",
    "stds = np.array(book['stds'])\n",
    "stds_ = abs(1-stds)\n",
    "stds_y = [abs(x-1) for x in np.mean(stds, axis=0)]\n",
    "ax1.errorbar(book[\"x_axis\"], stds_y, yerr=(np.max(stds_, axis=0)-stds_y, stds_y-np.min(stds_, axis=0)), ecolor=\"black\", color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.annotate(f'Mean: {str(round_sig(means.max(),5))} (best)', (0.5,-0.2), xycoords='axes fraction', textcoords='offset points', ha='center')\n",
    "plt.annotate(f'Std: {str(round_sig(min(list(stds.reshape(-1)), key=lambda x:abs(x-1)),5))} (best)', (0.5,-0.275), xycoords='axes fraction', textcoords='offset points', ha='center')\n",
    "plt.axvline(47, ymin=0, ymax=1, color=\"black\", linestyle='--', label=\"Nº of features \\nin the data\")\n",
    "plt.legend(loc='best')\n",
    "fig.savefig(join(img_dir, dir_name+variante+\"_mean_std.png\"), bbox_inches = 'tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(7.5,5))\n",
    "try:\n",
    "    rocs.pop(\"bkg\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for key in rocs.keys():\n",
    "    ax.set_xlabel('zdim')\n",
    "    ax.set_ylabel('AUC Score')\n",
    "    auc_score = np.array(rocs[key])\n",
    "    auc_score_y = np.mean(auc_score, axis=0)\n",
    "    ax.plot([x for x in range(5, 104, 5)], auc_score_y, label=key)\n",
    "    #ax.errorbar(book[\"x_axis\"], auc_score_y, yerr=(np.max(auc_score, axis=0)-auc_score_y, auc_score_y-np.min(auc_score, axis=0)), ecolor=\"black\", label = key)\n",
    "\n",
    "\n",
    "plt.axvline(47, ymin=0, ymax=1, color=\"black\", linestyle='--', label=\"Nº of features \\nin the data\")\n",
    "plt.legend(loc='best', ncol=1, bbox_to_anchor=(1, 1))\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "fig.savefig(join(img_dir, dir_name+variante+\"_aucScores.png\"), bbox_inches = 'tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.max(auc_score, axis=0)-auc_score_y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.std(auc_score, axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sanity check"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_trials = []\n",
    "for i,name in tqdm(enumerate(names), total=len(names)):\n",
    "    zdim = int(name.split(\"-\")[1].replace(' zdim ', '').replace(' ', ''))\n",
    "\n",
    "    study = optuna.load_study(study_name=name, storage=\"sqlite:///optimization.db\")\n",
    "    num_trials.append(len(study.trials))\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel('zdim')\n",
    "ax.set_ylabel('')\n",
    "ax.plot(book[\"x_axis\"], num_trials, label='Num. Trials')\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}